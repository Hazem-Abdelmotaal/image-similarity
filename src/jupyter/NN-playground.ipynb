{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.optimizers import RMSprop\n",
    "from keras import backend as K\n",
    "%matplotlib inline\n",
    "\n",
    "import pickle\n",
    "\n",
    "#\n",
    "from vgg16_siamese import vgg16_siamese, contrastive_loss\n",
    "import data_processing as data_p\n",
    "\n",
    "IMG_DIR = \"../../data/ssd-images/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training data\n",
    "train_batches = pickle.load(open('./batches/fast_train_batches.pkl', 'rb'))\n",
    "val_batches = pickle.load(open('./batches/fake_train_batches.pkl', 'rb'))\n",
    "test_batches = pickle.load(open('./batches/fast_test_batches.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            (None, 64, 64, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_9 (InputLayer)            (None, 64, 64, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_5 (Model)                 (None, 512)          16026432    input_8[0][0]                    \n",
      "                                                                 input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 1)            0           model_5[1][0]                    \n",
      "                                                                 model_5[2][0]                    \n",
      "==================================================================================================\n",
      "Total params: 16,026,432\n",
      "Trainable params: 1,311,744\n",
      "Non-trainable params: 14,714,688\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_shape = (64, 64, 3) # 64x64 rgb images\n",
    "siamese_network = vgg16_siamese(input_shape)\n",
    "\n",
    "siamese_network.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "500/500 [==============================] - 498s 996ms/step - loss: 0.2144 - accuracy: 0.7093 - val_loss: 0.1622 - val_accuracy: 0.7855\n",
      "Epoch 2/4\n",
      "500/500 [==============================] - 475s 950ms/step - loss: 0.1701 - accuracy: 0.7642 - val_loss: 0.1526 - val_accuracy: 0.8064\n",
      "Epoch 3/4\n",
      "500/500 [==============================] - 475s 950ms/step - loss: 0.1608 - accuracy: 0.7838 - val_loss: 0.1536 - val_accuracy: 0.8118\n",
      "Epoch 4/4\n",
      "500/500 [==============================] - 475s 950ms/step - loss: 0.1484 - accuracy: 0.8007 - val_loss: 0.1578 - val_accuracy: 0.8014\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa3c7f964e0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_accuracy(y_true, y_pred):\n",
    "    '''Compute classification accuracy with a fixed threshold on distances.\n",
    "    '''\n",
    "    pred = y_pred.ravel() < 0.5\n",
    "    return np.mean(pred == y_true)\n",
    "\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    '''Compute classification accuracy with a fixed threshold on distances.\n",
    "    '''\n",
    "    return K.mean(K.equal(y_true, K.cast(y_pred < 0.5, y_true.dtype)))\n",
    "\n",
    "def to_img_pairs(batch):\n",
    "    return data_p.df_to_img_pair(batch, IMG_DIR)\n",
    "\n",
    "train_img_batches = map(to_img_pairs, train_batches)\n",
    "val_img_batches = map(to_img_pairs, val_batches)\n",
    "test_img_batches = map(to_img_pairs, test_batches)\n",
    "\n",
    "rms = RMSprop()\n",
    "siamese_network.compile(loss=contrastive_loss, optimizer=rms, metrics=[accuracy])\n",
    "siamese_network.fit_generator(\n",
    "    train_img_batches,\n",
    "    steps_per_epoch=500,\n",
    "    epochs=4,\n",
    "    validation_data=val_img_batches,\n",
    "    validation_steps=500,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Accuracy on training set: 81.38%\n",
      "* Accuracy on test set: 79.16%\n"
     ]
    }
   ],
   "source": [
    "# compute final accuracy on training and test sets\n",
    "tr_img_batches = map(to_img_pairs, train_batches)\n",
    "te_img_batches = map(to_img_pairs, test_batches)\n",
    "\n",
    "\n",
    "# y_pred = model.predict(tr_img_batches)\n",
    "# tr_acc = compute_accuracy(tr_y, y_pred)\n",
    "# y_pred = model.predict(te_img_batches)\n",
    "# te_acc = compute_accuracy(te_y, y_pred)\n",
    "\n",
    "tr_loss, tr_acc = siamese_network.evaluate_generator(tr_img_batches, steps=50)\n",
    "te_loss, te_acc = siamese_network.evaluate_generator(te_img_batches, steps=100)\n",
    "\n",
    "print('* Accuracy on training set: %0.2f%%' % (100 * tr_acc))\n",
    "print('* Accuracy on test set: %0.2f%%' % (100 * te_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
